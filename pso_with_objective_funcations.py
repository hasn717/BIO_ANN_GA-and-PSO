# -*- coding: utf-8 -*-
"""PSO with Objective Funcations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4lHfB1Y0P_sc4c-Ji5oKzHrdkNSFYU8
"""

!pip install optproblems

import random
import math
import matplotlib.pyplot as plt
import numpy as np
from math import sin, sqrt
from  optproblems import cec2005
from  optproblems import Individual
import statistics
from datetime import datetime

class Problem:
  def __init__(self,f):
    self.f = f
  
  def evaluate(self,sol):
      func =  self.f(len(sol))
      solution=Individual(sol)
      func.evaluate(solution)
      return solution.objective_values

  def optimal(self,length):
    func = self.f(length)
    solutions = func.get_optimal_solutions()
    for sol in solutions:
      func.evaluate(sol) # return single solution
      return sol.objective_values
    return 0

class Particle:
    """A particle consists of two parts:
          The particleâ€™s location in space, ~x = hx1, x2, ...i. This is the equivalent, in evolutionary algorithms, of the individualâ€™s genotype.
          The particleâ€™s velocity, ~v = hv1, v2, ...i. This is the speed and direction at which the particle is traveling each timestep"""
    def __init__(self, bounds,w,c1,c2,num_variables):
        self.particle_velocity = []                     
        self.particle_position = []                     
        self.best_position = [] # particle best position        
        self.best_pos_fitness = float("inf") # initialize best position fitness 
        self.particle_pos_fitness = float("inf")
        self.num_variables = num_variables                 
        self.w=w
        self.c1=c1
        self.c2=c2

       
        for i in range(self.num_variables):
        #define random initial position and velocity
            self.particle_position.append(random.uniform(bounds[0],bounds[1]))
            self.particle_velocity.append(random.uniform(-1, 1))
    #Evalaute using Objective Funcations 
    def evaluate(self, objective_fun):
        self.particle_pos_fitness = objective_fun.evaluate(
            self.particle_position)
        
        if self.particle_pos_fitness < self.best_pos_fitness:
          #update best position and fitness
          self.best_position = self.particle_position
          self.best_pos_fitness = self.particle_pos_fitness

    def update_position(self, bounds):
        for i in range(self.num_variables):
            self.particle_position[i] = self.particle_position[i] + \
                self.particle_velocity[i]

            # checking with the upper bounds
            if self.particle_position[i] > bounds[1]:
                self.particle_position[i] = bounds[1]
            # checking with the lower bounds
            if self.particle_position[i] < bounds[0]:
                self.particle_position[i] = bounds[0]

    # update particles by cheinging their positions and velocity depending on information of best position and global best position
    """Velocity update
        At each iteration, velocity of a particle is changed according to four criteria
        1-	 Own best position so far (cognitive component): a coefficient (ð›½) is used to
             weight this component, usually called the cognitive weight
        2-	Informantsâ€™ best position (social component): a coefficient (ð›¾) is used to
            weight this component, called the social weight
        3-	Global best position (part of social component): weighted by a coefficient (ð›¿)
            called the global weight
        4-	An inertia principle by which we consider the previous velocity, using an
            inertia weight (ð›¼)"""  
    def update_velocity(self, global_best_pos, fittest_informant):
        self.fittest_informant=fittest_informant
        for i in range(self.num_variables):
            r1 = random.random()
            r2 = random.random()

            cognitive_velocity = self.c1*r1 * \
                (self.best_position[i] -
                 self.particle_position[i])
            #update social velocity by informant fittest
            social_velocity = self.c2*r2 * \
                (fittest_informant.particle_position[i] - self.particle_position[i])
            globalVelocity=self.c2*r2 * \
                (global_best_pos[i] - self.particle_position[i])    
            self.particle_velocity[i] = self.w*self.particle_velocity[i] + \
                cognitive_velocity + social_velocity+globalVelocity

#Evaluate swarm and return the fittest particle based on previous position
def get_current_best(swarm, objective_fun):
    fitnesses = [assess_fitness(x.particle_position, objective_fun) for x in swarm]
    best_value = min(fitnesses)
    best_index = fitnesses.index(best_value)
    return swarm[best_index]

def assess_fitness(individual, objective_fun):
    "Determines the fitness of an individual using the given problem"
    return objective_fun.evaluate(individual)

class PSO():
    def __init__(self, objective_fun, bounds, num_particles, iterations,num_informants,w,c1,c2,num_variables):
        self.num_informants = num_informants #informants: set of particles with which a particle shares information
        self.global_best_fitness = float("inf")
        self.global_best_pos = []
        global_best_positions=[]
        swarm_particle = []
        self.history = []
        for i in range(num_particles):# Particle size= number of particles
            swarm_particle.append(Particle(bounds,w,c1,c2,num_variables))

        for i in range(iterations):
            for j in range(num_particles):
                swarm_particle[j].evaluate(objective_fun)
                
                if swarm_particle[j].particle_pos_fitness < self.global_best_fitness:
                  self.global_best_pos = list(swarm_particle[j].particle_position)
                  self.global_best_fitness = float(swarm_particle[j].particle_pos_fitness)
            self.history.append(self.global_best_fitness)

            ## iterate to assign particle informants and update the position and velocity for particle with gbest and ibest
            for j in range(num_particles):
                #informants find fittest
                informants = np.random.choice(swarm_particle, self.num_informants)
                if swarm_particle[j] not in informants:
                  np.append(informants, swarm_particle[j])
                  #find the best in swarm by informants    
                fittest_informant = get_current_best(informants, objective_fun)

                swarm_particle[j].update_velocity(self.global_best_pos, fittest_informant)
                swarm_particle[j].update_position(bounds)       
        
    def get_fittest_global_Pos(self):
      return self.global_best_pos
    def get_global_best_fitness(self):
      return self.global_best_fitness

"""# Evaluation & Experiments"""

f1 = Problem(cec2005.F1)
f2 = Problem(cec2005.F2)
f3 = Problem(cec2005.F3)
f4 = Problem(cec2005.F4)
f5 = Problem(cec2005.F5)
f6 = Problem(cec2005.F6)
f8 = Problem(cec2005.F8)

def run_experiment(OBJECTIVE_FUNC,VECTOR_LENGTH,BOUNDS, NUM_PARTICLES, ITERATIONS, NUM_INFORMANTS,W,C1,C2,runs=50):
  ## Evaluation 
  evaluation = {
      'accuracies': [],
      'timedRuns': [],
      'runs': [],
  }
  for run in range(runs):
    print('RUN ',run+1)
    start_time = datetime.now()
    pso = PSO(OBJECTIVE_FUNC, BOUNDS, NUM_PARTICLES, ITERATIONS, NUM_INFORMANTS,W,C1,C2,VECTOR_LENGTH)
    score,solution=pso.get_global_best_fitness(), pso.get_fittest_global_Pos()
    delta_time = datetime.now() - start_time 
    timeTaken = delta_time.total_seconds() * 1000
    accuracy = (score / OBJECTIVE_FUNC.optimal(VECTOR_LENGTH)) * 100

    evaluation['timedRuns'].append(timeTaken)
    evaluation['accuracies'].append(accuracy)
    evaluation['runs'].append(pso.history)

    print('Time taken->',timeTaken, 'ms')
    print('Fittest individual->',solution)
    print('Accuracy ->',accuracy,'%')
    print('#####################################')
  return evaluation

## Experiments parameetrs 
experiment_runs = 25

"""## Experiment 1

### F1
"""

## Running Experiment 1 for F1
## Hyperparameters
print('F1')
OBJECTIVE_FUNC=f1
VECTOR_LENGTH = 10
BOUNDS= (-100,100)
NUM_PARTICLES =50
ITERATIONS=30
NUM_INFORMANTS=2
W = 0.1  # inertia constant
C1 = 0.1   # cognative constant
C2 = 1    # social constant
exp_f1 = run_experiment(OBJECTIVE_FUNC,VECTOR_LENGTH,BOUNDS,NUM_PARTICLES,ITERATIONS,NUM_INFORMANTS,W,C1,C2,experiment_runs)

"""### F4"""

## Running Experiment 1 for F4
## Hyperparameters
print('F4')
OBJECTIVE_FUNC=f4
VECTOR_LENGTH = 10
BOUNDS= (-100,100)
NUM_PARTICLES =150
ITERATIONS=50
NUM_INFORMANTS=10
W = 0.1  # inertia constant
C1 = 0.1   # cognative constant
C2 = 1    # social constant
exp_f4 = run_experiment(OBJECTIVE_FUNC,VECTOR_LENGTH,BOUNDS,NUM_PARTICLES,ITERATIONS,NUM_INFORMANTS,W,C1,C2,experiment_runs)

"""### F8"""

## Running Experiment 1 for F8
## Hyperparameters
print('F8')
OBJECTIVE_FUNC=f8
VECTOR_LENGTH = 10
BOUNDS= (-32,32)
NUM_PARTICLES =10
ITERATIONS=10
NUM_INFORMANTS=2
W = 0.2  # inertia constant
C1 = 0.1   # cognative constant
C2 = 0.2    # social constant
exp_f8 = run_experiment(OBJECTIVE_FUNC,VECTOR_LENGTH,BOUNDS,NUM_PARTICLES,ITERATIONS,NUM_INFORMANTS,W,C1,C2,experiment_runs)

"""## Analysis"""

## Compare F1,F4,F8 Accuracy
x_axis = [i for i in range(experiment_runs)]
plt.plot(x_axis, exp_f1['accuracies'])
plt.plot(x_axis, exp_f4['accuracies'])
plt.plot(x_axis, exp_f8['accuracies'])

plt.title('Accuracy vs Runs')
plt.xlabel('Runs')
plt.ylabel('Accuracy(%)')
plt.legend(['F1', 'F4','F8'])
plt.show()

"""### F1"""

## F1 Convergence Single Run
x_axis = [i for i in range(len(exp_f1['runs'][0]))]
plt.plot(x_axis, exp_f1['runs'][0])
plt.title('Convergence')
plt.xlabel('Number of iterations')
plt.ylabel('Fitness')
plt.show()

"""### F4"""

## F4 Convergence Single Run
x_axis = [i for i in range(len(exp_f4['runs'][0]))]
plt.plot(x_axis, exp_f4['runs'][0])
plt.title('Convergence')
plt.xlabel('Number of iterations')
plt.ylabel('Fitness')
plt.show()

"""### F8"""

## F8 Convergence Single Run
x_axis = [i for i in range(len(exp_f8['runs'][0]))]
plt.plot(x_axis, exp_f8['runs'][0])
plt.title('Convergence')
plt.xlabel('Number of iterations')
plt.ylabel('Fitness')
plt.show()

best_exp_f1 = np.max(exp_f1['accuracies'])
print('Best F1 Accuracy->',best_exp_f1)
best_exp_f4 = np.max(exp_f4['accuracies'])
print('Best F4 Accuracy->',best_exp_f4)
best_exp_f8 = np.max(exp_f8['accuracies'])
print('Best F8 Accuracy->',best_exp_f8)

worst_exp_f1 = np.min(exp_f1['accuracies'])
print('Worst F1 Accuracy->',worst_exp_f1)
worst_exp_f4 = np.min(exp_f4['accuracies'])
print('Worst F4 Accuracy->',worst_exp_f4)
worst_exp_f8 = np.min(exp_f8['accuracies'])
print('Worst F8 Accuracy->',worst_exp_f8)

median_exp_f1 = np.median(exp_f1['accuracies'])
print('median F1 Accuracy->',median_exp_f1)
median_exp_f4 = np.median(exp_f4['accuracies'])
print('median F4 Accuracy->',median_exp_f4)
median_exp_f8 = np.median(exp_f8['accuracies'])
print('median F8 Accuracy->',median_exp_f8)

from statistics import mean 
t1 = mean(exp_f1['timedRuns'])
t4 =  mean(exp_f4['timedRuns'])
t8 =  mean(exp_f8['timedRuns'])
print(t1,t4,t8)

reliability_exp_f1 = mean(exp_f1['accuracies'])
accuracy_exp_f1 = mean(exp_f1['accuracies'][0:20])
efficiency_exp_f1 = mean(exp_f1['timedRuns'])

print("Reliability",reliability_exp_f1)
print("Accuracy",accuracy_exp_f1)
print("Efficiency",efficiency_exp_f1)

reliability_exp_f4 = mean(exp_f4['accuracies'])
accuracy_exp_f4 = mean(exp_f4['accuracies'][0:20])
efficiency_exp_f4 = mean(exp_f4['timedRuns'])

print("Reliability",reliability_exp_f4)
print("Accuracy",accuracy_exp_f4)
print("Efficiency",efficiency_exp_f4)

reliability_exp_f8 = mean(exp_f8['accuracies'])
accuracy_exp_f8 = mean(exp_f8['accuracies'][0:20])
efficiency_exp_f8 = mean(exp_f8['timedRuns'])

print("Reliability",reliability_exp_f8)
print("Accuracy",accuracy_exp_f8)
print("Efficiency",efficiency_exp_f8)

"""## Experiment 2

### F1
"""

## Running Experiment F1
## Hyperparameters
print('F1')
OBJECTIVE_FUNC=f1
VECTOR_LENGTH = 10
BOUNDS= (-100,100)
NUM_PARTICLES =100
ITERATIONS=50
NUM_INFORMANTS=2
W = 0.1  # inertia constant
C1 = 0.1   # cognative constant
C2 = 1    # social constant
exp_f1 = run_experiment(OBJECTIVE_FUNC,VECTOR_LENGTH,BOUNDS,NUM_PARTICLES,ITERATIONS,NUM_INFORMANTS,W,C1,C2,experiment_runs)

"""### F4"""

## Running Experiment F4
## Hyperparameters
print('F4')
OBJECTIVE_FUNC=f4
VECTOR_LENGTH = 10
BOUNDS= (-100,100)
NUM_PARTICLES =400
ITERATIONS=30
NUM_INFORMANTS=20
W = 0.1  # inertia constant
C1 = 0.1   # cognative constant
C2 = 1    # social constant
exp_f4 = run_experiment(OBJECTIVE_FUNC,VECTOR_LENGTH,BOUNDS,NUM_PARTICLES,ITERATIONS,NUM_INFORMANTS,W,C1,C2,experiment_runs)

"""### F8"""

## Running Experiment F8
## Hyperparameters
print('F8')
OBJECTIVE_FUNC=f8
VECTOR_LENGTH = 10
BOUNDS= (-32,32)
NUM_PARTICLES =20
ITERATIONS=10
NUM_INFORMANTS=2
W = 0.1  # inertia constant
C1 = 0.1   # cognative constant
C2 = 1    # social constant
exp_f8 = run_experiment(OBJECTIVE_FUNC,VECTOR_LENGTH,BOUNDS,NUM_PARTICLES,ITERATIONS,NUM_INFORMANTS,W,C1,C2,experiment_runs)

## Compare F1,F4,F8 Accuracy

x_axis = [i for i in range(experiment_runs)]
plt.plot(x_axis, exp_f1['accuracies'])
plt.plot(x_axis, exp_f4['accuracies'])
plt.plot(x_axis, exp_f8['accuracies'])

plt.title('Accuracy vs Runs')
plt.xlabel('Runs')
plt.ylabel('Accuracy(%)')
plt.legend(['F1', 'F4','F8'])
plt.show()

"""## Analysis

### F1
"""

## F1 Convergence Single Run
x_axis = [i for i in range(len(exp_f1['runs'][0]))]
plt.plot(x_axis, exp_f1['runs'][0])
plt.title('Convergence')
plt.xlabel('Number of iterations')
plt.ylabel('Fitness')
plt.show()

"""### F4"""

## F4 Convergence Single Run
x_axis = [i for i in range(len(exp_f4['runs'][0]))]
plt.plot(x_axis, exp_f4['runs'][0])
plt.title('Convergence')
plt.xlabel('Number of iterations')
plt.ylabel('Fitness')
plt.show()

"""### F8"""

## F8 Convergence Single Run
x_axis = [i for i in range(len(exp_f8['runs'][0]))]
plt.plot(x_axis, exp_f8['runs'][0])
plt.title('Convergence')
plt.xlabel('Number of iterations')
plt.ylabel('Fitness')
plt.show()

best_exp_f1 = np.max(exp_f1['accuracies'])
print('Best F1 Accuracy->',best_exp_f1)
best_exp_f4 = np.max(exp_f4['accuracies'])
print('Best F4 Accuracy->',best_exp_f4)
best_exp_f8 = np.max(exp_f8['accuracies'])
print('Best F8 Accuracy->',best_exp_f8)

worst_exp_f1 = np.min(exp_f1['accuracies'])
print('Worst F1 Accuracy->',worst_exp_f1)
worst_exp_f4 = np.min(exp_f4['accuracies'])
print('Worst F4 Accuracy->',worst_exp_f4)
worst_exp_f8 = np.min(exp_f8['accuracies'])
print('Worst F8 Accuracy->',worst_exp_f8)

median_exp_f1 = np.median(exp_f1['accuracies'])
print('median F1 Accuracy->',median_exp_f1)
median_exp_f4 = np.median(exp_f4['accuracies'])
print('median F4 Accuracy->',median_exp_f4)
median_exp_f8 = np.median(exp_f8['accuracies'])
print('median F8 Accuracy->',median_exp_f8)

from statistics import mean 

t1 = mean(exp_f1['timedRuns'])
t4 =  mean(exp_f4['timedRuns'])
t8 =  mean(exp_f8['timedRuns'])

print(t1,t4,t8)

reliability_exp2_f1 = mean(exp_f1['accuracies'])
accuracy_exp2_f1 = mean(exp_f1['accuracies'][0:20])
efficiency_exp2_f1 = mean(exp_f1['timedRuns'])

print("Reliability",reliability_exp2_f1)
print("Accuracy",accuracy_exp2_f1)
print("Efficiency",efficiency_exp2_f1)

reliability_exp2_f4 = mean(exp_f4['accuracies'])
accuracy_exp2_f4 = mean(exp_f4['accuracies'][0:20])
efficiency_exp2_f4 = mean(exp_f4['timedRuns'])

print("Reliability",reliability_exp2_f4)
print("Accuracy",accuracy_exp2_f4)
print("Efficiency",efficiency_exp2_f4)

reliability_exp2_f8 = mean(exp_f8['accuracies'])
accuracy_exp2_f8 = mean(exp_f8['accuracies'][0:20])
efficiency_exp2_f8 = mean(exp_f8['timedRuns'])

print("Reliability",reliability_exp2_f8)
print("Accuracy",accuracy_exp2_f8)
print("Efficiency",efficiency_exp2_f8)