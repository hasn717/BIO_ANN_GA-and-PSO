# -*- coding: utf-8 -*-
"""MLP_Draft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wa45f6uqAutIRMNuhEtYkwXQ5kwYDSO0

# Implement a multi-layer ANN architecture
"""

import numpy as np
import sys
import struct
import math
import os
import matplotlib.pyplot as plt
import pandas as pd 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import minmax_scale
from enum import Enum
import inspect

class Activation:

  functions = []
  id = 0

  def __init__(self,function,derivative,name=''):
    self.function = function
    self.derivative = derivative
    self.name = name
    self.id = Activation.id
    self.functions.append(self)
    Activation.id+=1
  def __repr__(self):
        return f"<Activtion Function> {self.name}"

  def __str__(self):
        return f"<Activtion Function> {self.name}"

## Define activation functions here
sigmoid = Activation(lambda z : 1. / (1. + np.exp(-np.clip(z, -250, 250))),lambda z:  z * (1. - z),name='Sigmoid')
relu = Activation(lambda z: np.maximum(z, 0),lambda z: z > 0,name='RELU')
tanh= Activation(lambda z: np.tanh(z), lambda z: 1-np.tanh(z)**2,name='Tanh')
linear = Activation(lambda z: z,lambda z: 1,name='Linear')
one = Activation(lambda z: 1,lambda z: 0,name='One')

"""# New Section"""



class Encoding:
  @staticmethod
  def onehot(y,classes):
    onehot = np.zeros((classes, y.shape[0]))
    for idx, val in enumerate(y.astype(int)):
          onehot[val, idx] = 1.
    return onehot.T


## Cost function Enum
class CostFunction(Enum):
     LOGISTIC_LOSS = 0
     CROSS_ENTROPY = 1
     MEAN_SQUARED_ROOT=2
## GD enums
class GradientDescent(Enum):
     BATCH = 0
     MINIBATCH = 1
     STOCHASTIC = 2

## Learning rate mode enum
class LearningRate(Enum):
     CONSTANT = 0
     SCHEDULE = 1

# Custom Schedules for learning rate
# it takes an integer denoting the iteration index (starting at 1) and returns a float representing the learning rate to be used for that iteration. 
# implementing the Cosine Annealing Schedule in the example
class CosineAnnealingSchedule():
    def __init__(self, min_lr, max_lr, cycle_length):
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.cycle_length = cycle_length

    def __call__(self, iteration):
        if iteration <= self.cycle_length:
            unit_cycle = (1 + math.cos(iteration * math.pi / self.cycle_length)) / 2
            adjusted_cycle = (unit_cycle * (self.max_lr - self.min_lr)) + self.min_lr
            return adjusted_cycle
        else:
            return self.min_lr

class MLP:
  def __init__(self,NoInputNodes,NhiddenLayers=3,epochs=100,lr=0.001,minibatchSize=1,seed=None,shuffle=True,NhiddenNeurons = 2,
               activation = relu,costFunction: CostFunction=CostFunction.LOGISTIC_LOSS, GD:GradientDescent=GradientDescent.BATCH
               , learningRateMode: LearningRate = LearningRate.CONSTANT, l2 = 0.1, max_lr =0.01,min_lr=0.001
               ):
    self.NoInputNodes=NoInputNodes or 0
    self.NhiddenNeurons = NhiddenNeurons or 0
    self.learningRateMode = learningRateMode ## Standard/Decay
    self.epochs = epochs
    self.random = np.random.RandomState(seed)
    self.lr =lr
    self.shuffle = shuffle
    self.minibatchSize = minibatchSize
    self.NhiddenLayers = NhiddenLayers
    self.l2 = l2 # regulizer
    self.activation = activation
    self.pickedCostFunction = costFunction
    self.pickedGD = GD
    self.evaluation = {'cost': [], 'train_acc': [], 'valid_acc': []}
    self.min_lr = min_lr
    self.max_lr = max_lr
    self.info()

  def info(self):
    print('Neural Network Parameters:')
    print('Input nodes:',self.NoInputNodes if self.NoInputNodes > 0 else 'No changes - same as input size')
    print('Hidden nodes/layer:',self.NhiddenNeurons if self.NhiddenNeurons > 0 else '')
    print('Epochs:',self.epochs)
    print('GD:',GradientDescent(self.pickedGD))
    if(self.pickedGD is GradientDescent.MINIBATCH):  
      print('Mini-Batch Size:',self.minibatchSize)  
    print('Learning Rate:',self.lr)  
    print('Shuffle:',self.shuffle)
    print('Activation Function:',self.activation)  
    print('Learning Rate Mode:',LearningRate(self.learningRateMode))
    if(self.learningRateMode is LearningRate.SCHEDULE):
      print('Min Learning Rate:',self.min_lr) 
      print('Max Learning Rate:',self.max_lr)
    print('Cost Function:',CostFunction(self.pickedCostFunction))
    print('#############################')


 

  def forward(self,X):
    zHidden = np.dot(X,self.wHidden) + self.biasHidden
    activationHidden = self.activation.function(zHidden)
    zOutput = np.dot(activationHidden,self.wOutput) + self.biasOutput
    activationOutput = self.activation.function(zOutput)
    return zHidden, activationHidden, zOutput, activationOutput


  ## Cost function : Logistic Loss  
  def logistic_loss(self,y_enc,output):
    # Logistic Loss
    L2_term = (self.l2 *
                (np.sum(self.wHidden ** 2.) +
                np.sum(self.wOutput ** 2.)))

    term1 = -y_enc * (np.log(output + 1e-5)) # addition of a small constant to avoid zero division (undefined cost value)
    term2 = (1. - y_enc) * np.log(1. - output + 1e-5) 
    cost = np.sum(term1 - term2) + L2_term
    return cost
  
  ## Cost function : Cross Entropy
  def cross_entropy(self,y_enc,output):
    loss=-np.sum(y_enc*np.log(output+1e-5)) # addition of a small constant to avoid zero division (undefined cost value)
    return loss/float(output.shape[0])

  ## Cost function : Mean Squared Root
  def mean_squared_error(self,y_enc,output):
    diff = output - y_enc
    differences_squared = diff ** 2
    return differences_squared.mean()


  def predict(self,X):
    zHidden, activationHidden, zOutput, activationOutput = self.forward(X)
    return np.argmax(zOutput, axis=1)


  def fit(self, X_train, y_train,X_actual,y_actual):


    if(self.NoInputNodes > 0):
      X_train = X_train[:,0:self.NoInputNodes]
      X_actual = X_actual[:,0:self.NoInputNodes]


    numberOfFeatures = X_train.shape[1]
    numberOfLabels = np.unique(y_train).shape[0] 

 

    ## Evalution prepartion
    epoch_strlen = len(str(self.epochs))  
    self.evaluation = {'cost': [], 'train_acc': [], 'valid_acc': []} ## reset last evaulation

    ## Initialize a decay learning rate function
    if(self.learningRateMode is LearningRate.SCHEDULE):
      schedule = CosineAnnealingSchedule(min_lr=self.min_lr, max_lr=self.max_lr, cycle_length=self.epochs)
    
    layerData = np.empty(self.NhiddenLayers, dtype=object)
    for epoch in range(self.epochs):
      ## Forward Pass
      for layer in range(self.NhiddenLayers):
        wOutputSize = numberOfFeatures
        if(layer == self.NhiddenLayers - 1):
          wOutputSize = numberOfLabels  # last layer
        # print('forward',layer,wOutputSize)
        if(len(layerData) > 0 and layerData[layer]): # <--  Retrieve saved weights --> 
          self.biasHidden = layerData[layer].get('biasHidden')
          self.wHidden = layerData[layer].get('wHidden')
          self.biasOutput = layerData[layer].get('biasOutput')
          self.wOutput = layerData[layer].get('wOutput')
        else:  # <--  Initialize weights --> 
          ## Weights input -> hidden
          self.biasHidden = np.zeros(self.NhiddenNeurons)
          self.wHidden = self.random.normal(loc=0.0,scale=0.1,size=(numberOfFeatures, self.NhiddenNeurons))
          ## Weights hidden -> output 
          self.biasOutput = np.zeros(wOutputSize)
          self.wOutput = self.random.normal(loc=0.0, scale=0.1, size=(self.NhiddenNeurons, wOutputSize ))
        
        ## Save layer data 
        layerData[layer] = {
            'wHidden': self.wHidden,
            'biasHidden': self.biasHidden,
            'biasOutput': self.biasOutput,
            'wOutput': self.wOutput
        }

        batches = np.arange(X_train.shape[0])
        if self.shuffle:
          self.random.shuffle(batches)
    
        ## Mini batch learning -> Use mini batches of the samples specified by the miniBatchSize
        if self.pickedGD is GradientDescent.MINIBATCH:
          start = 0
          stop = batches.shape[0] - self.minibatchSize + 1
          step = self.minibatchSize
        ## Stochastic GD -> Pick a single random sample
        elif self.pickedGD is GradientDescent.STOCHASTIC:
          randomPick = np.random.randint(0,batches.shape[0] - 1) # Pick random sample
          start = randomPick
          stop = randomPick + 1
          step = 1 
          self.minibatchSize = step # Override batch size by step size (one in case of SGD)
        ## Batch GD -> Use the whole batch at once
        elif self.pickedGD is GradientDescent.BATCH:
          start = 0
          stop = 1
          step = 1
          self.minibatchSize = batches.shape[0]
        else:
          print('No GD picked!')
          return

        
        for start_idx in range(start, stop,step):
            batch_idx = batches[start_idx:start_idx + self.minibatchSize]
            zHidden, activationHidden, zOutput, activationOutput = self.forward(X_train[batch_idx])
            layerData[layer].update({'zHidden':zHidden, 'activationHidden':activationHidden,'zOutput':zOutput,'activationOutput':activationOutput})
            layerData[layer].update({'batch_idx':batch_idx})

      ## Backward pass
      for layer in range(self.NhiddenLayers -1 ,-1,-1):
        wOutputSize = numberOfFeatures 
        if(layer == self.NhiddenLayers -1):
          wOutputSize = numberOfLabels   # last layer
        # print('backward ',layer, wOutputSize)

  
        ## One hot encoding 
        y_train_encoded = Encoding.onehot(y_train,wOutputSize) ##TODO we should have the encoding before starting the epoch?

        # <-- back propagation --> 
        self.biasHidden = layerData[layer]['biasHidden']
        self.wHidden = layerData[layer]['wHidden']
        self.biasOutput = layerData[layer]['biasOutput']
        self.wOutput = layerData[layer]['wOutput']
        
        zHidden, activationHidden, zOutput, activationOutput = layerData[layer].get('zHidden'),layerData[layer].get('activationHidden'),layerData[layer].get('zOutput'),layerData[layer].get('activationOutput')
        batch_idx = layerData[layer].get('batch_idx')
        actualDiffPredicted = activationOutput - y_train_encoded[batch_idx]

        activationDerivativeHidden = self.activation.derivative(activationHidden)

        diffHidden = (np.dot(actualDiffPredicted,self.wOutput.T) * activationDerivativeHidden)

        ## Gradients hidden <- output (target labels)
        wHiddenGradient = np.dot(X_train[batch_idx].T,diffHidden )
        biasHiddenGradient = np.sum(diffHidden, axis=0)

        # Regularization and weight updates
        diff_w_hidden = (wHiddenGradient + self.l2*self.wHidden)
        diff_bias_hidden = biasHiddenGradient # bias is not regularized

        ## Adjust learning rate if using a decay function
        if(self.learningRateMode is LearningRate.SCHEDULE):
          self.lr = schedule(epoch)

        self.wHidden -= self.lr * diff_w_hidden
        self.biasHidden -= self.lr * diff_bias_hidden


        ## Gradients input (class labels) <- hidden
        wOutputGradient = np.dot(activationHidden.T, actualDiffPredicted)
        biasOutputGradient = np.sum(actualDiffPredicted, axis=0)


        # Regularization and weight updates
        diff_w_output = (wOutputGradient + self.l2*self.wOutput)
        diff_bias_output = biasOutputGradient  # bias is not regularized
        
        self.wOutput -= self.lr * diff_w_output
        self.biasOutput -= self.lr * diff_bias_output
 

      ### <-- Evaluation -->
      # After each epoch during training
      zHidden, activationHidden, zOutput, activationOutput = self.forward(X_train)

      ## Calculate cost function depending on the picked cost function 
      if(self.pickedCostFunction is CostFunction.LOGISTIC_LOSS):
        cost = self.logistic_loss(y_train_encoded,activationOutput)
      elif(self.pickedCostFunction is CostFunction.CROSS_ENTROPY):
        cost = self.cross_entropy(y_train_encoded,activationOutput)
      elif(self.pickedCostFunction is CostFunction.MEAN_SQUARED_ROOT):
        cost = self.mean_squared_error(y_train_encoded,activationOutput)
      else:
        raise Exception('Cost function is not defined!')
      y_train_pred = self.predict(X_train)
      y_valid_pred = self.predict(X_actual)
      train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /
                    X_train.shape[0])
      valid_acc = ((np.sum(y_actual == y_valid_pred)).astype(np.float) /
                    X_actual.shape[0])
      print('\r%0*d/%d | Cost: %.6f '
                        '| Accuracy: %.2f%%/%.2f%% ' %
                        (epoch_strlen, epoch+1, self.epochs, cost,
                        train_acc*100, valid_acc*100))

      self.evaluation['cost'].append(cost)
      self.evaluation['train_acc'].append(train_acc)
      self.evaluation['valid_acc'].append(valid_acc)
    # self.info()

# """# Data Preparation"""

# data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',sep=',')
# ## Discretization
# data=data.replace(to_replace="M",value=1)
# data=data.replace(to_replace="B",value=0)
# ## Normalization (scaling) 
# data[:] = minmax_scale(data)

# # data

# """# Training """

# target_data = data.iloc[:,1].to_numpy() ## Target class (Select column 2)
# features_data = data.iloc[:,[i for  i in range(2,data.shape[1])]].to_numpy() ## Features (Select column 3 - 32)
# X_train, X_test, y_train, y_test = train_test_split(features_data,target_data, test_size=0.2,random_state=40) ## Split

# print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))

# ## Hyperparameters:
# NoInputNodes =0
# NhiddenLayers = 2
# NhiddenNeurons=10
# epochs= 100
# lr=0.001
# minibatchSize=100
# shuffle=True
# seed=2
# activation=relu
# costFunction=CostFunction.MEAN_SQUARED_ROOT
# GD=GradientDescent.MINIBATCH
# learningRateMode=LearningRate.SCHEDULE
# MIN_LR=0.001
# MAX_LR=0.01

# nn = MLP(
#         NhiddenNeurons=NhiddenNeurons,
#     NoInputNodes=NoInputNodes,
#     NhiddenLayers=NhiddenLayers, 
#         epochs=epochs, 
#         lr=lr,
#         minibatchSize=minibatchSize, 
#         shuffle=shuffle,
#         seed=seed,
#         activation=activation,
#         costFunction=costFunction,
#          GD=GD,
#         learningRateMode=learningRateMode,
#         min_lr=MIN_LR,
#          max_lr=MAX_LR
#          )

# nn.fit(X_train=X_train, 
#        y_train=y_train,
#        X_actual=X_test,
#        y_actual=y_test)

"""# Graphing"""

##visualize the results using Matplotlib
def plot_cost_vs_epochs(nn):
  plt.clf()
  plt.plot(range(nn.epochs), nn.evaluation['cost'])
  plt.ylabel('Cost')
  plt.xlabel('Epochs')
  plt.savefig('cost_vs_epoch.png', dpi=300)
  # plt.show()




def plot_training_vs_validation_acc(nn):
  plt.clf()
  plt.plot(range(nn.epochs), nn.evaluation['train_acc'], 
          label='Training')
  plt.plot(range(nn.epochs), nn.evaluation['valid_acc'], 
          label='Validation', linestyle='--')
  plt.ylabel('Accuracy')
  plt.xlabel('Epochs')
  plt.legend(loc='lower right')
  plt.savefig('training_vs_validation.png', dpi=300)
  # plt.show()

